---
title: "📃 텍스트 토큰화"
tags:
    - 딥러닝
    - 데이터
    - 전처리
date: "2025-11-13"
thumbnail: "/assets/img/thumbnail/토큰.png"
---

# 📃 텍스트 토큰화
---
## 📚 토큰화란?
텍스트 토큰화(Tokenization)는 문장을 기계가 이해할 수 있는 작은 단위(토큰)로 쪼개는 과정입니다.
예를 들어, "나는 인공지는이 좋다"라는 문장을 토큰화 한다면 ["나", "는", "인공지능", "이", "좋다"]로 쪼개지게 됩니다.
토큰화의 실제 내부과정은 꽤 복잡하고, 사용하는 모델에 따라 쪼개는 방식이 달라집니다.
## 📚 토큰화의 흐름
### 1. 텍스트 정제 (Preprocessing / Normalization)
1. 여러 형태의 공백들을 표준 공백으로 변환합니다.
2. 영어의 경우 의미 차이가 크게 없다면 소문자로 변환합니다.
3. 특수문자, 이모지, URL, 기호 등은 제거 혹은 변경을 합니다.
4. "don't"와 같은 약어를 ["do", "n't"] 같이 정규화를 시킵니다.


### 2. 단어 단위 토큰화 (Whitespace Tokenizer, Rule-based)
전통적인 NLP(자연어 처리)는 단순한 방식이 많았습니다.
1. 공백 기준 분리
"I love deep learning" > ["I", "love", "deep", "learning"]
2. 점/콤마/마침표 분리
“Hello, world!” > ["Hello", ",", "world", "!"]

### 3. 서브워드 토큰화 (BPE, WordPiece, SentencePiece)
오늘날의 딥러닝 모델들은 거의 서브워드 기반 토큰화를 사용합니다.
이 방식은 오늘날의 신조어, 오타, 드문 단어를 처리하기 위해 등장했습니다.
예를 들어, "unbelievable" 라는 단어를 토큰화 한다면
전통방식 : ["unbelievable"] 한 단어 전체 > **OOV 발생**
BPE 방식 : ["un", "believe", "able"] 출현 빈도가 높은 서브워드를 생성하여 사전으로 사용

OOV란 사전에 포함되지 않은 단어들을 뜻합니다.

### 4. 토큰 > ID 매핑 (Vocabulary Lookup)
토큰으로 변경을 했다면 각각 쪼개진 토큰들을 고유 숫자로 변환합니다.
"I love AI" > tokenizer > ["I", "love", "AI"] > vocab lookup > [101, 4112, 2081]

## 📚 모델별 토큰화 방식

| 라이브러리/모델                | 토큰화 방식               | 특징            |
| ----------------------- | -------------------- | ------------- |
| **NLTK**                | 공백/규칙 기반             | 단순, 고전 NLP    |
| **SpaCy**               | 규칙 기반 + 예외 처리        | 영어 최강         |
| **KoNLPy**              | 형태소 분석               | 한국어 전용        |
| **Word2Vec / FastText** | (토큰화 없음) → 사용자 제공    | 벡터 학습용        |
| **SentencePiece**       | BPE / Unigram        | LLM 표준, 공백 포함 |
| **BERT**                | WordPiece            | ## 접두어        |
| **GPT 계열**              | Byte-Level BPE       | 범용 LLM 표준     |
| **T5 / XLNet**          | Unigram              | 확률 기반         |
| **LLaMA / Mistral**     | SentencePiece 기반 BPE | LLM 주요 방식     |
