---
title: "📃 텍스트 벡터화"
tags:
    - 딥러닝
    - 데이터
    - 전처리
date: "2025-11-13"
thumbnail: "/assets/img/thumbnail/벡터.jpg"
---

# 📃 텍스트 벡터화
---
## 📚 벡터화란?
텍스트 벡터화는 토큰화를 거쳐 문자열을 ID로 변환하고 그 ID를 벡터로 변환하는 과정입니다.
## 📚 벡터화의 종류
### 1. 전통적 벡터화(초기 NLP 방식)
이 방식들은 단어의 의미를 거의 반영하지 못합니다.
대신 단순하게 텍스트를 숫자로 바꿉니다.
1. 원-핫 인코딩 (One-hot Encoding)
단어 사전 크기 = vocab_size
각 단어는 해당 위치만 1인 벡터
예를 들어 단어사전이 5개라면
cat → [1,0,0,0,0]
dog → [0,1,0,0,0]
단순하지만 벡터가 희소(sparse)하고 의미를 표현하지 못합니다
2. Bag-of-Words (BoW)
문장에 등장하는 "단어의 빈도"만 기록.
문장: "I love AI. I love deep learning"
사전: I, love, AI, deep, learning
BoW: [2, 2, 1, 1, 1]
빠르고 쉽지만 단어 순서·문맥 정보가 사라지는 큰 단점
3. TF-IDF
단어의 빈도 + 중요도(weight) 반영
희귀한 단어일수록 점수를 높게 부여.
BoW보다 똑똑하지만 똑같이 문맥 정보 없음


### 2. 딥러닝 기반 벡터화(임베딩 Embedding)
현대 NLP에서 사용하는 방식입니다.
단어/문장에 의미가 담긴 dense vector를 생성합니다.
1. Word2Vec
비슷한 문맥에서 사용되는 단어는 비슷한 의미를 가집니다.
그래서 king, queen / gold, mining / cat, dog, animal
이런 관계가 벡터 공간에 자연스럽게 반영됩니다.
의미 반영이 빠르고 계산 빠르며 문맥 변화는 반영하지 못함
2. FastText
"playing" → "play", "lay", "ing", 같이 한 단어를 서브워드 단위로도 학습합니다.
오타/신조어에도 강하고 한국어처럼 조사/어미 많은 언어에 좋습니다.
3. GloVe
전 세계 코퍼스에서 함께 등장하는 통계(공동 등장 확률)를 분석해 의미 벡터를 만드는 방식입니다.
Word2Vec보다 전역적 의미를 반영합니다.
4. BERT / GPT 임베딩 (문맥 기반)
문맥에 따라 같은 단어라도 전혀 다른 벡터가 나옵니다.
bank = 은행 / bank = 강가
이 두 문맥의 벡터는 완전히 다름.
문맥까지 이해하고 가장 똑똑한 임베딩 모든 최신 언어모델이 사용